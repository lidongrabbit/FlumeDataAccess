etl1_mme02.channels = mme02_c1
etl1_mme02.sources = s1 s2 s3
etl1_mme02.sinks =k1

# Define spooling source
etl1_mme02.sources.s1.type = spooldir
etl1_mme02.sources.s1.channels = mme02_c1
etl1_mme02.sources.s1.spoolDir = /data1/mme/02/partA
etl1_mme02.sources.s1.batchSize = 10000
etl1_mme02.sources.s1.ignorePattern = ^(.)*\\.tmp$
etl1_mme02.sources.s1.consumeOrder = youngest
etl1_mme02.sources.s1.deletePolicy = immediate
etl1_mme02.sources.s1.interceptors = i1
etl1_mme02.sources.s1.interceptors.i1.type = regex_extractor
etl1_mme02.sources.s1.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme02.sources.s1.interceptors.i1.serializers = e1
#etl1_mme02.sources.s1.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme02.sources.s1.interceptors.i1.serializers.e1.name = key

etl1_mme02.sources.s2.type = spooldir
etl1_mme02.sources.s2.channels = mme02_c1
etl1_mme02.sources.s2.spoolDir = /data1/mme/02/partB
etl1_mme02.sources.s2.batchSize = 10000
etl1_mme02.sources.s2.ignorePattern = ^(.)*\\.tmp$
etl1_mme02.sources.s2.consumeOrder = youngest
etl1_mme02.sources.s2.deletePolicy = immediate
etl1_mme02.sources.s2.interceptors = i1
etl1_mme02.sources.s2.interceptors.i1.type = regex_extractor
etl1_mme02.sources.s2.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme02.sources.s2.interceptors.i1.serializers = e1
#etl1_mme02.sources.s2.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme02.sources.s2.interceptors.i1.serializers.e1.name = key

etl1_mme02.sources.s3.type = spooldir
etl1_mme02.sources.s3.channels = mme02_c1
etl1_mme02.sources.s3.spoolDir = /data1/mme/02/partC
etl1_mme02.sources.s3.batchSize = 10000
etl1_mme02.sources.s3.ignorePattern = ^(.)*\\.tmp$
etl1_mme02.sources.s3.consumeOrder = youngest
etl1_mme02.sources.s3.deletePolicy = immediate
etl1_mme02.sources.s3.interceptors = i1
etl1_mme02.sources.s3.interceptors.i1.type = regex_extractor
etl1_mme02.sources.s3.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme02.sources.s3.interceptors.i1.serializers = e1
#etl1_mme02.sources.s3.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme02.sources.s3.interceptors.i1.serializers.e1.name = key

# Define a kafka channel
etl1_mme02.channels.mme02_c1.type = org.apache.flume.channel.kafka.KafkaChannel
etl1_mme02.channels.mme02_c1.kafka.bootstrap.servers = oc-etl-data-new-066:6667,oc-etl-data-new-067:6667,oc-etl-data-new-068:6667,oc-etl-data-new-069:6667,oc-etl-data-new-070:6667,oc-etl-data-new-071:6667,oc-etl-data-new-072:6667,oc-etl-data-new-073:6667,oc-etl-data-new-074:6667,oc-etl-data-new-075:6667
etl1_mme02.channels.mme02_c1.kafka.topic = ocspin
etl1_mme02.channels.mme02_c1.parseAsFlumeEvent = false

#================================================
etl1_mme03.channels = mme03_c1
etl1_mme03.sources = s4 s5 s6
etl1_mme03.sinks =k1

etl1_mme03.sources.s4.type = spooldir
etl1_mme03.sources.s4.channels = mme03_c1
etl1_mme03.sources.s4.spoolDir = /data1/mme/03/partA
etl1_mme03.sources.s4.batchSize = 10000
etl1_mme03.sources.s4.ignorePattern = ^(.)*\\.tmp$
etl1_mme03.sources.s4.consumeOrder = youngest
etl1_mme03.sources.s4.deletePolicy = immediate
etl1_mme03.sources.s4.interceptors = i1
etl1_mme03.sources.s4.interceptors.i1.type = regex_extractor
etl1_mme03.sources.s4.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme03.sources.s4.interceptors.i1.serializers = e1
#etl1_mme03.sources.s4.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme03.sources.s4.interceptors.i1.serializers.e1.name = key

etl1_mme03.sources.s5.type = spooldir
etl1_mme03.sources.s5.channels = mme03_c1
etl1_mme03.sources.s5.spoolDir = /data1/mme/03/partB
etl1_mme03.sources.s5.batchSize = 10000
etl1_mme03.sources.s5.ignorePattern = ^(.)*\\.tmp$
etl1_mme03.sources.s5.consumeOrder = youngest
etl1_mme03.sources.s5.deletePolicy = immediate
etl1_mme03.sources.s5.interceptors = i1
etl1_mme03.sources.s5.interceptors.i1.type = regex_extractor
etl1_mme03.sources.s5.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme03.sources.s5.interceptors.i1.serializers = e1
#etl1_mme03.sources.s5.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme03.sources.s5.interceptors.i1.serializers.e1.name = key

etl1_mme03.sources.s6.type = spooldir
etl1_mme03.sources.s6.channels = mme03_c1
etl1_mme03.sources.s6.spoolDir = /data1/mme/03/partC
etl1_mme03.sources.s6.batchSize = 10000
etl1_mme03.sources.s6.ignorePattern = ^(.)*\\.tmp$
etl1_mme03.sources.s6.consumeOrder = youngest
etl1_mme03.sources.s6.deletePolicy = immediate
etl1_mme03.sources.s6.interceptors = i1
etl1_mme03.sources.s6.interceptors.i1.type = regex_extractor
etl1_mme03.sources.s6.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme03.sources.s6.interceptors.i1.serializers = e1
#etl1_mme03.sources.s6.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme03.sources.s6.interceptors.i1.serializers.e1.name = key

# Define a kafka channel
etl1_mme03.channels.mme03_c1.type = org.apache.flume.channel.kafka.KafkaChannel
etl1_mme03.channels.mme03_c1.kafka.bootstrap.servers = oc-etl-data-new-066:6667,oc-etl-data-new-067:6667,oc-etl-data-new-068:6667,oc-etl-data-new-069:6667,oc-etl-data-new-070:6667,oc-etl-data-new-071:6667,oc-etl-data-new-072:6667,oc-etl-data-new-073:6667,oc-etl-data-new-074:6667,oc-etl-data-new-075:6667
etl1_mme03.channels.mme03_c1.kafka.topic = ocspin
etl1_mme03.channels.mme03_c1.parseAsFlumeEvent = false

#================================================
etl1_mme01.channels = mme01_c1
etl1_mme01.sources = s7 s8 s9
etl1_mme01.sinks =k1

etl1_mme01.sources.s7.type = spooldir
etl1_mme01.sources.s7.channels = mme01_c1
etl1_mme01.sources.s7.spoolDir = /data2/mme/01/partA
etl1_mme01.sources.s7.batchSize = 10000
etl1_mme01.sources.s7.ignorePattern = ^(.)*\\.tmp$
etl1_mme01.sources.s7.consumeOrder = youngest
etl1_mme01.sources.s7.deletePolicy = immediate
etl1_mme01.sources.s7.interceptors = i1
etl1_mme01.sources.s7.interceptors.i1.type = regex_extractor
etl1_mme01.sources.s7.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme01.sources.s7.interceptors.i1.serializers = e1
#etl1_mme01.sources.s7.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme01.sources.s7.interceptors.i1.serializers.e1.name = key

etl1_mme01.sources.s8.type = spooldir
etl1_mme01.sources.s8.channels = mme01_c1
etl1_mme01.sources.s8.spoolDir = /data2/mme/01/partB
etl1_mme01.sources.s8.batchSize = 10000
etl1_mme01.sources.s8.ignorePattern = ^(.)*\\.tmp$
etl1_mme01.sources.s8.consumeOrder = youngest
etl1_mme01.sources.s8.deletePolicy = immediate
etl1_mme01.sources.s8.interceptors = i1
etl1_mme01.sources.s8.interceptors.i1.type = regex_extractor
etl1_mme01.sources.s8.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme01.sources.s8.interceptors.i1.serializers = e1
#etl1_mme01.sources.s8.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme01.sources.s8.interceptors.i1.serializers.e1.name = key

etl1_mme01.sources.s9.type = spooldir
etl1_mme01.sources.s9.channels = mme01_c1
etl1_mme01.sources.s9.spoolDir = /data2/mme/01/partC
etl1_mme01.sources.s9.batchSize = 10000
etl1_mme01.sources.s9.ignorePattern = ^(.)*\\.tmp$
etl1_mme01.sources.s9.consumeOrder = youngest
etl1_mme01.sources.s9.deletePolicy = immediate
etl1_mme01.sources.s9.interceptors = i1
etl1_mme01.sources.s9.interceptors.i1.type = regex_extractor
etl1_mme01.sources.s9.interceptors.i1.regex = [A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|[A-Za-z0-9]*\\|([A-Za-z0-9]*)\\|.*
etl1_mme01.sources.s9.interceptors.i1.serializers = e1
#etl1_mme01.sources.s9.interceptors.i1.serializers.e1.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
etl1_mme01.sources.s9.interceptors.i1.serializers.e1.name = key

# Define a kafka channel
etl1_mme01.channels.mme01_c1.type = org.apache.flume.channel.kafka.KafkaChannel
etl1_mme01.channels.mme01_c1.kafka.bootstrap.servers = oc-etl-data-new-066:6667,oc-etl-data-new-067:6667,oc-etl-data-new-068:6667,oc-etl-data-new-069:6667,oc-etl-data-new-070:6667,oc-etl-data-new-071:6667,oc-etl-data-new-072:6667,oc-etl-data-new-073:6667,oc-etl-data-new-074:6667,oc-etl-data-new-075:6667
etl1_mme01.channels.mme01_c1.kafka.topic = ocspin
etl1_mme01.channels.mme01_c1.parseAsFlumeEvent = false